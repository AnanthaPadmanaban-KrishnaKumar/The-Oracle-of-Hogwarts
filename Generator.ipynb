{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y2_AFxM9JknM"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U langchain-community pinecone-client openai tqdm python-dotenv pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "from pinecone.grpc import PineconeGRPC as Pinecone\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "from openai import OpenAI\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import getpass"
      ],
      "metadata": {
        "id": "37y1xc2wJoWM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone_api_key = getpass.getpass(\"Enter Pinecone API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM0R_sr_Jxgb",
        "outputId": "b36e4e0e-ac08-48ca-ce25-ed5b2a374714"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Pinecone API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = getpass.getpass(\"Enter OpenAI API key: \")\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "openai_client = OpenAI()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FmWyaxXKDmd",
        "outputId": "b0c76da9-a6d7-4770-fd31-a4b785da6666"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"potteroracle\"\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "index = pc.Index(index_name)\n",
        "model_name = \"text-embedding-3-small\""
      ],
      "metadata": {
        "id": "ZXI6Lg6pKe04"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed(documents: list[str], model_name: str) -> list[list[float]]:\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of documents using OpenAI.\n",
        "\n",
        "    Args:\n",
        "    documents (list[str]): A list of documents to embed.\n",
        "    model_name (str): The name of the OpenAI model to use.\n",
        "\n",
        "    Returns:\n",
        "    list[list[float]]: A list of embeddings.\n",
        "    \"\"\"\n",
        "    response = openai_client.embeddings.create(input=documents, model=model_name)\n",
        "    document_embeddings = [result.embedding for result in response.data]\n",
        "    return document_embeddings"
      ],
      "metadata": {
        "id": "CX-He-5sKJaF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_docs(query: str, top_k: int, model_name: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Retrieve documents matching a query from the Pinecone index.\n",
        "\n",
        "    Args:\n",
        "    query (str): The query string.\n",
        "    top_k (int): The number of top documents to retrieve.\n",
        "\n",
        "    Returns:\n",
        "    list[str]: A list of document texts matching the query.\n",
        "    \"\"\"\n",
        "    print(f\"Getting docs with {index_name}\")\n",
        "    query_embedding = embed([query], model_name=model_name)[0]\n",
        "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
        "    documents = [result[\"metadata\"]['text'] for result in results[\"matches\"]]\n",
        "    return documents"
      ],
      "metadata": {
        "id": "NZMEERLVKQyP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query: str, retrieved_docs: list[str], model_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate an answer to the query based on the retrieved documents using OpenAI.\n",
        "\n",
        "    Args:\n",
        "    query (str): The query string.\n",
        "    retrieved_docs (list[str]): The list of retrieved documents.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated answer.\n",
        "    \"\"\"\n",
        "    context = \"\\n\\n\".join(retrieved_docs)\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"The following is a context and a question. Based on the context, provide a detailed and accurate answer to the question.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "QpT9baujKWEZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_sample_query(query: str, top_k: int, embed_model_name: str, completion_model_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Execute a sample query and print the generated answer.\n",
        "\n",
        "    Args:\n",
        "    query (str): The query string.\n",
        "    top_k (int): The number of top documents to retrieve.\n",
        "    embed_model_name (str): The name of the embedding model to use.\n",
        "    completion_model_name (str): The name of the completion model to use.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    retrieved_docs = get_docs(query=query, top_k=top_k, model_name=embed_model_name)\n",
        "    answer = generate_answer(query=query, retrieved_docs=retrieved_docs, model_name=completion_model_name)\n",
        "    print(\">>>\")\n",
        "    print(f\"Question: {query}\")\n",
        "    print(f\"Answer: {answer}\")\n",
        "    print(\">>>\")"
      ],
      "metadata": {
        "id": "ZTBo4WDcKTem"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Execute a sample query\n",
        "    execute_sample_query(\n",
        "        query=\"What is the function of the Marauder's Map?\",\n",
        "        top_k=5,\n",
        "        embed_model_name=model_name,\n",
        "        completion_model_name=\"gpt-4\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsXaq61CKSap",
        "outputId": "7b91ee3b-8791-43b8-eea9-7255d886ce47"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting docs with potteroracle\n",
            ">>>\n",
            "Question: What is the function of the Marauder's Map?\n",
            "Answer: The Marauder's Map is a magical map of Hogwarts castle and grounds. It shows every detail of the castle along with the names of everyone inside labeled as tiny moving ink dots. It also reveals shortcuts and secret passages. Most importantly, it lets the user know if someone is approaching them by showing the movement of people around the castle. This makes the map especially useful for those who want to move around the castle without being detected.\n",
            ">>>\n"
          ]
        }
      ]
    }
  ]
}